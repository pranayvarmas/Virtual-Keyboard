# Object Detection and Object Tracking Using HSV Color Space

## HSV->

### Hue
Hue corresponds to the colour components(base pigment) hence just by selecting a range of hue u can select any colour(0-360)

### Saturation
Saturation is the amount of colour(depth of pigment)(dominance of hue)(0-100%)

### Value
Value  is the brightness of the colour(0-100%)

![hsv](https://upload.wikimedia.org/wikipedia/commons/thumb/f/f2/HSV_color_solid_cone.png/800px-HSV_color_solid_cone.png)



## Code
### To detect an object in an image:

```
import cv2
import numpy as np

def nothing(x):                                                 #call back function,dummy function in this case
    pass

cv2.namedWindow("Tracking")                                     #creating window named tracking
cv2.createTrackbar("LH", "Tracking", 0, 255, nothing)           # Creating trackbars for lower and upper hue,saturation and value
cv2.createTrackbar("LS", "Tracking", 0, 255, nothing)           
cv2.createTrackbar("LV", "Tracking", 0, 255, nothing)
cv2.createTrackbar("UH", "Tracking", 255, 255, nothing)
cv2.createTrackbar("US", "Tracking", 255, 255, nothing)
cv2.createTrackbar("UV", "Tracking", 255, 255, nothing)

while True:                                                 #to keep running program to keep getting new trackbar positions until escaped
    frame = cv2.imread('smarties.png')

    hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)            #converting defualt BGR format of frame to HSV and storing in hsv variable

    l_h = cv2.getTrackbarPos("LH", "Tracking")              # getting trackbar positions for getting lower and upper bounds of HSV
    l_s = cv2.getTrackbarPos("LS", "Tracking")
    l_v = cv2.getTrackbarPos("LV", "Tracking")

    u_h = cv2.getTrackbarPos("UH", "Tracking")
    u_s = cv2.getTrackbarPos("US", "Tracking")
    u_v = cv2.getTrackbarPos("UV", "Tracking")

    l_b = np.array([l_h, l_s, l_v])                         # creating arrays storing bounds in H,S,V format
    u_b = np.array([u_h, u_s, u_v])

    mask = cv2.inRange(hsv, l_b, u_b)                       #mask for thresholding hsv image to only get regions in the bounds set by trackbar

    res = cv2.bitwise_and(frame, frame, mask=mask)          # applying mask on coloured image using bitwise_and function and storing it as result=res

    cv2.imshow("frame", frame)                              #displaying orignal image,mask and resultant image
    cv2.imshow("mask", mask)
    cv2.imshow("res", res)

    key = cv2.waitKey(1)
    if key == 27:
        break

cv2.destroyAllWindows()
```
---

### To detect an object in a video:

```
import cv2
import numpy as np

def nothing(x):                                                 #dummy function
    pass

cap = cv2.VideoCapture(0);

cv2.namedWindow("Tracking")                                         #creating a window named 'tracking'
cv2.createTrackbar("LH", "Tracking", 0, 255, nothing)               #creating trackbars for lower and upper bounds of hsv
cv2.createTrackbar("LS", "Tracking", 0, 255, nothing)
cv2.createTrackbar("LV", "Tracking", 0, 255, nothing)
cv2.createTrackbar("UH", "Tracking", 255, 255, nothing)
cv2.createTrackbar("US", "Tracking", 255, 255, nothing)
cv2.createTrackbar("UV", "Tracking", 255, 255, nothing)

while True:                                         #to keep running program to keep getting new trackbar positions and frames from video until escaped
    _, frame = cap.read()                           #getting frame from video

    hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV).   # coverting BGR to HSV and storing in hsv variable

    l_h = cv2.getTrackbarPos("LH", "Tracking")      #getting trackbar positions for getting lower and upper bounds of HSV
    l_s = cv2.getTrackbarPos("LS", "Tracking")
    l_v = cv2.getTrackbarPos("LV", "Tracking")

    u_h = cv2.getTrackbarPos("UH", "Tracking")
    u_s = cv2.getTrackbarPos("US", "Tracking")
    u_v = cv2.getTrackbarPos("UV", "Tracking")

    l_b = np.array([l_h, l_s, l_v])                 # creating arrays storing bounds in H,S,V format
    u_b = np.array([u_h, u_s, u_v])

    mask = cv2.inRange(hsv, l_b, u_b)               #mask for thresholding hsv image to only get regions in the bounds set by trackbar

    res = cv2.bitwise_and(frame, frame, mask=mask)  #applying mask on coloured image using bitwise_and function and storing it as result=res

    cv2.imshow("frame", frame)                      #displaying orignal image,mask and resultant image(which due to loop give a video output) 
    cv2.imshow("mask", mask)
    cv2.imshow("res", res)

    key = cv2.waitKey(1)
    if key == 27:
        break

cap.release()
cv2.destroyAllWindows()
```
## references

[youtube video](https://www.youtube.com/watch?v=3D7O_kZi8-o&list=PLS1QulWo1RIa7D1O6skqDQ-JZ1GGHKK-K&index=14)
## 14

# Simple Image Thresholding
Thresholding is a popular segmentation technique used for separating an object from its background.Process of thresholding involves comparing each pixel of an image with a predefined threshold value.

First output of threshold is ret but here we have written _ because we won't be using this variable anywhere.
First parameter in threshold is image or source
Second parameter in threshold method is threshold value and third parameter is maximum threshold value
Fourth parameter is threshold type(it can be of several types)

**cv.THRESH_BINARY** - We are comparing each and every value of pixel to threshold value and if pixel value < threshold value then pixel value = 0 and if pixel value > threshold value then pixel value = 255(white pixel)

**cv.THRESH_BINARY_INV** -Gives result opposite to cv.THRESH_BINARY.If pixel value < threshold value then pixel value = 255(white pixel)
and if pixel value > threshold value then pixel value = 0

**cv.THRESH_TRUNC** - Upto threshold, value of pixels will not be changed.After threshold, pixel values would be equal to threshold value.

**cv.THRESH_TOZERO** - When pixel value < threshold value, the value assigned to pixel will be zero(black pixel)

**cv.THRESH_TOZERO_INV** - Opposite of cv.THRESH_TOZERO.If pixel value > threshold value then pixel value will be zero otherwise pixel value would be equal to threshold value.

```python

import cv2 as cv
import numpy as np
img = cv.imread('gradient.png',0)

_, th1 = cv.threshold(img, 50, 255, cv.THRESH_BINARY)
_, th2 = cv.threshold(img, 200, 255, cv.THRESH_BINARY_INV)
_, th3 = cv.threshold(img, 127, 255, cv.THRESH_TRUNC)
_, th4 = cv.threshold(img, 127, 255, cv.THRESH_TOZERO)
_, th5 = cv.threshold(img, 127, 255, cv.THRESH_TOZERO_INV)

cv.imshow("Image", img)
cv.imshow("th1", th1)
cv.imshow("th2", th2)
cv.imshow("th3", th3)
cv.imshow("th4", th4)
cv.imshow("th5", th5)

cv.waitKey(0)
cv.destroyAllWindows()

```

## 15
# ADAPTIVE THRESHOLDING

In **simple thresholding**, the threshold value is **global**, i.e., it is same for all the pixels in the image. **Adaptive thresholding is the method where the threshold value is calculated for smaller regions and therefore, there will be different threshold values for different regions.**

In OpenCV, you can perform Adaptive threshold operation on an image using the method **adaptiveThreshold()** of the **Imgproc** class. 

Following is the syntax of this method.

`adaptiveThreshold(src, dst, maxValue, adaptiveMethod, thresholdType, blockSize, C)`

This method accepts the following parameters −

**src**− An object of the class Mat representing the source (input) image.

**dst** − An object of the class Mat representing the destination (output) image.

**maxValue** − A variable of double type representing the value that is to be given if pixel value is more than the threshold value.

**adaptiveMethod** − A variable of integer the type representing the adaptive method to be used. This will be either of the following two values

- **ADAPTIVE_THRESH_MEAN_C** − threshold value is the mean of neighborhood area.

- **ADAPTIVE_THRESH_GAUSSIAN_C** − threshold value is the weighted sum of neighborhood values where weights are a Gaussian window.

**thresholdType** − A variable of integer type representing the type of threshold to be used.

**blockSize** − A variable of the integer type representing size of the pixelneighborhood used to calculate the threshold value.

**C** − A variable of double type representing the constant used in the both methods (subtracted from the mean or weighted mean).

## Code
```python

import cv2 as cv
import numpy as np

img = cv.imread('sudoku.png',0)
_, th1 = cv.threshold(img, 127, 255, cv.THRESH_BINARY)
th2 = cv.adaptiveThreshold(img, 255, cv.ADAPTIVE_THRESH_MEAN_C, cv.THRESH_BINARY, 11, 2);
th3 = cv.adaptiveThreshold(img, 255, cv.ADAPTIVE_THRESH_GAUSSIAN_C, cv.THRESH_BINARY, 11, 2);

cv.imshow("Image", img)
cv.imshow("THRESH_BINARY", th1)
cv.imshow("ADAPTIVE_THRESH_MEAN_C", th2)
cv.imshow("ADAPTIVE_THRESH_GAUSSIAN_C", th3)

cv.waitKey(0)
cv.destroyAllWindows()
```

## references

- [tutorialspoint](https://www.tutorialspoint.com/opencv/opencv_adaptive_threshold.htm)
- [youtube video](https://www.youtube.com/watch?v=Zf1F4cz8GHU&list=PLS1QulWo1RIa7D1O6skqDQ-JZ1GGHKK-K&index=17)

## 16
```python
import cv2
from matplotlib import pyplot as plt

img = cv2.imread('lena.jpg',-1) # opencv reads an image in bgr format
cv2.imshow('image',img) # the image will be shown correctly as bgr format is used
cv2.waitKey(0)
cv2.destroyAllWindows()
plt.imshow(img)
plt.show()
# matplotlib reads in rgb format, therefore this will show image incorrectly
img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) # to convert bgr format to rgb using opencv
plt.imshow(img)
plt.show()
# now img with true colors is shown
plt.imshow(img)
plt.xticks([]), plt.yticks([]) # hides the x and y value marks or axes in the image
plt.show()
```

```python
# using the code in the last tutorial to get 6 different images and to show them in a single matplotlib window

import cv2
import numpy as np
from matplotlib import pyplot as plt

img = cv2.imread('gradient.png', 0) # image involves different pixel values starting from 0 to 255

_, th1 = cv2.threshold(img, 50, 255, cv2.THRESH_BINARY)
_, th2 = cv2.threshold(img, 200, 255, cv2.THRESH_BINARY_INV)
_, th3 = cv2.threshold(img, 127, 255, cv2.THRESH_TRUNC)
_, th4 = cv2.threshold(img, 127, 255, cv2.THRESH_TOZERO)
_, th5 = cv2.threshold(img, 127, 255, cv2.THRESH_TOZERO_INV)

titles = ['Original', 'BINARY', 'BINARY_INV', 'TRUNC', 'TOZERO', 'TOZERO_INV'] # an array of titles
images = [img, th1, th2, th3, th4, th5] # array of matrices containing the information of different images

for i in range(6):
    plt.subplot(2,3, i+1) # arguments of this method are - no of rows, no of columns, index of the image (that is to be assigned)
    plt.imshow(images[i], 'gray') # image is picked of index i, when thresholding is done grayscale images are used so just put gray in second parameter
    plt.title([titles[i]]) # this method gives the title which is given as argument to the corresponding image
    plt.xticks([]), plt.yticks([])

plt.show()
```
##17
## Morphological Transformation
```Python

import cv2                                                                           
import numpy as np                                                                   # import various libraries
from matplotlib import pyplot as plt                                                  

img = cv2.imread('smarties.png', cv2.IMREAD_GRAYSCALE)                               # reading image named smarties is imported in grayscale
_, mask = cv2.threshold(img, 220, 255, cv2.THRESH_BINARY_INV)                        # the image is masked to convert to black or white

kernal = np.ones((5,5), np.uint8)                                                    # a kernal is created to use as a filter for pixels

dilation = cv2.dilate(mask, kernal, iterations=2)                                    # a pixel element is '1' if atleast one pixel under the kernel is '1'
erosion = cv2.erode(mask, kernal, iterations=1)                                      # a pixel in the original image (either 1 or 0) will be considered 1 only if all the pixels under the kernel is 1, otherwise it is eroded (made to zero)
opening = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernal)                             # opening is just another name of erosion followed by dilation
closing = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernal)                            # closing is reverse of opening, dilation followed by erosion
mg = cv2.morphologyEx(mask, cv2.MORPH_GRADIENT, kernal)                              # it is the difference between dilation and erosion of an image
th = cv2.morphologyEx(mask, cv2.MORPH_TOPHAT, kernal)                                # it is the difference between input image and opening of the image

titles = ['image', 'mask', 'dilation', 'erosion', 'opening', 'closing', 'mg', 'th']  # defining array named 'titles' which has array elements as titles of images.
images = [img, mask, dilation, erosion, opening, closing, mg, th]                    # defining array named 'images' which has array the images.
for i in range(8):                                                                   # displaying the images
    plt.subplot(2, 4, i+1), plt.imshow(images[i], 'gray')
    plt.title(titles[i])
    plt.xticks([]),plt.yticks([])

plt.show()
```

## 18

``` python
import cv2
import numpy as np
from matplotlib import pyplot as plt

img = cv2.imread('lena.jpg')
img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)    #because we are using matplotlib

kernel = np.ones((5, 5), np.float32)/25       #creating the kernel for homogenous(average) filter
dst = cv2.filter2D(img, -1, kernel)           # using cv2.filter2D() to convolve over the image with the above defined kernel. 
                                              #It is a general function can be used with a customized kernel.
                                              #Here -1 is the depth(no. of channels) of the output image. (-1 means depth same as input image)
blur = cv2.blur(img, (5, 5))                  #Opencv has a default function for average filter(the one we implemented above)
                                              #kernel size can be adjusted.
gblur = cv2.GaussianBlur(img, (5, 5), 0)      #In gaussian blur we convolve over the image woth a different kind of kernel
                                              #(it is weighted in both x and y directions with the center being the highest weighted.)
                                              #Kernel size can be adjusted. 0 is the standard deviation of kernel values in x-direction.
median = cv2.medianBlur(img, 5)               #Median filter replaces the current pixel with the median value among the values in the kernel. 
                                              #Kernel size can be adjusted. Median filter excellent for salt and pepper noise. 
bilateralFilter = cv2.bilateralFilter(img, 9, 75, 75)   #Bilateral filter is helpful when you need to smoothen the image but keep the edges sharp.
                                                        # 9 is the diameter of pixel neighbourhood(equivalent to kernel size)
                                                        #next two parameters are sigmacolour and sigmaspace respectively(explained in the link below)
titles = ['image', '2D Convolution', 'blur', 'GaussianBlur', 'median', 'bilateralFilter']   
images = [img, dst, blur, gblur, median, bilateralFilter]
                                                                                #plotting all the images in a single window.
for i in range(6):
    plt.subplot(2, 3, i+1), plt.imshow(images[i], 'gray')
    plt.title(titles[i])
    plt.xticks([]),plt.yticks([])

plt.show()
```
[sigmacolour and sigmaspace](https://docs.opencv.org/2.4/modules/imgproc/doc/filtering.html?highlight=bilateralfilter#bilateralfilter)
## 19
# Image Gradients and Edge Detection 
```Python
#Code for image gradient and Edge detection
#Image Gradient is directional change the intensity or color in an image

import numpy as np                                 #
import cv2                                         #importing numpy,opencv and matplotlib libraries.
import matplotlib.pyplot as plt                    #

img=cv2.imread('messi.jpg',cv2.IMREAD_GRAYSCALE)   #reading image named 'messi.jpg' from directory as a grayscale image

lap=cv2.Laplacian(img,cv2.CV_64F,ksize=3)          #Applying Laplacian gradient on img(1st arg) with datatype as 64 bit 
                                                   #float(2nd arg) and kernel size as 3(3rd arg).
                                                   #64 bit float is used due to the negative slop induced during conversion 
                                                   #from white to black while applying Laplacian gradient.
                                                   #Kernel size can be changed as 1,3,5,etc.                                                 
                                                   
sobelX=cv2.Sobel(img,cv2.CV_64F,1,0)               #Applying Sobel method of image gradient on image(1st arg) with datatype 
                                                   #64 bit float in x direction(3rd arg),not in y direction(4th arg)
                                                   #3rd and 4th arguments stand for SobleX and SobelY application. 
                                                   
sobelY=cv2.Sobel(img,cv2.CV_64F,0,1)               #Applying sobel method in y direction with similar arguments as before. 

lap=np.uint8(np.absolute(lap))                     #Taking absolute value of transformed laplacian image and converting back   
                                                   #the 'lap' image to unsighned 8 bit integer. 
sobelX=np.uint8(np.absolute(sobelX))               #similar process for sobelX image.
sobelY=np.uint8(np.absolute(sobelY))               #similar process for sobelY image.

sobelcombined=cv2.biwise_or(sobelX,sobely)         #combining both sobleX and sobleY using bitwise 'or' operator.

titles=['image','Laplacian','sobelX','sobelY',     #defining array named 'titles' which has array elements as titles of 
         'sobelcombined']                          #displaying images.
images=[img,lap,sobelX,sobelY,                     #defining array named 'images' which has array elemnts as displaying 
         sobelcombined]                            #images.
                                                   
for i in range(5):                                 #introducing for loop
  plt.subplot(2,3,i+1)                             #for displaying multiple images simultaneously in the format 2x3.         
  plt.imshow(images[i],'gray')                     #showing images in grayscale mode.
  plt.title(titles[i])                             #giving titles.
  plt.xticks([]),plt.yticks([])                    #giving zero ticks.
plt.show()                                         #displaying of images


#when we apply sobelx,sobely methods, change in intensities are along x,y directions respectively.
#combining sobelX and sobelY method gives us better result in edge detection.
#we can also include kernel size in sobelx and sobely method.
#increasing kernel size in Laplacian gradient deteriorates edge detection result.
```
## 20
# Canny Edge Detector
The Canny edge detector is used to detect multiple edges in an image (using multi-stage algorithm). Canny Edge detection gives more precise edges and reduces noise as compared to Laplacian or Sobel method. The algorithm is mainly gray-scale. 
The detection algorithm is of essentially 5 steps.

### 1. Noise reduction
   Since the mathematics involved behind the scene are mainly based on derivatives (cf. Step 2: Gradient calculation), edge detection results are highly sensitive to image noise. One way to get rid of the noise on the image, is by applying Gaussian blur to smooth it. To do so, image convolution technique is applied with a Gaussian Kernel (3x3, 5x5, 7x7 etc…). The kernel size depends on the expected blurring effect. Basically, the smallest the kernel, the less visible is the blur. In our example, we will use a 5 by 5 Gaussian kernel.
   ![](https://miro.medium.com/max/1050/1*YpLYVBomcYNNbwncG5iP9Q.png)

### 2. Gradient calculation
   The Gradient calculation step detects the edge intensity and direction by calculating the gradient of the image using edge detection operators. Edges correspond to a change of pixels’ intensity. To detect it, the easiest way is to apply filters that highlight this intensity change in both directions: horizontal (x) and vertical (y)
When the image is smoothed, the derivatives Ix and Iy w.r.t. x and y are calculated. It can be implemented by convolving I with Sobel kernels Kx and Ky, respectively. 
![](https://miro.medium.com/max/1254/1*ZCyKWsmDoj6V-dNwKlKxyA.png)
   In the result some of the edges are thick and others are thin. Non-Max Suppression step will help us mitigate the thick ones.
Moreover, the gradient intensity level is between 0 and 255 which is not uniform. The edges on the final result should have the same intensity (i-e. white pixel = 255).

### 3. Non-max suppression
   The principle is simple: the algorithm goes through all the points on the gradient intensity matrix and finds the pixels with the maximum value in the edge directions. Let's take an example:
   ![](https://miro.medium.com/max/1094/1*CWrXNSbe7s4qSFr5vylyvQ.png)
   The upper left corner red box present on the above image, represents an intensity pixel of the Gradient Intensity matrix being processed. The corresponding edge direction is represented by the orange arrow with an angle of -pi radians (+/-180 degrees).
   ![](https://miro.medium.com/max/804/1*K-gnZg4_VPk57Xs0XflIrg.png)
   The edge direction is the orange dotted line (horizontal from left to right). The purpose of the algorithm is to check if the pixels on the same direction are more or less intense than the ones being processed. In the example above, the pixel (i, j) is being processed, and the pixels on the same direction are highlighted in blue (i, j-1) and (i, j+1). If one those two pixels are more intense than the one being processed, then only the more intense one is kept. Pixel (i, j-1) seems to be more intense, because it is white (value of 255). Hence, the intensity value of the current pixel (i, j) is set to 0. If there are no pixels in the edge direction having more intense values, then the value of the current pixel is kept.
   
### 4. Double Threshold
   The double threshold step aims at identifying 3 kinds of pixels: strong (high enough intensity to contribute to edge), weak (intensity value not high but not small enough to be considered irrelevant for edge detection), and non-relevant (non-relevant for the edge). Now you can see what the double thresholds holds for:
- High threshold is used to identify the strong pixels (intensity higher than the high threshold)
- Low threshold is used to identify the non-relevant pixels (intensity lower than the low threshold)
- All pixels having intensity between both thresholds are flagged as weak and the Hysteresis mechanism (next step) will help us identify the ones that could be considered as strong and the ones that are considered as non-relevant.
![](https://miro.medium.com/max/1332/1*FF6b8FJ2oppREoh9T-hdfA.png)

### 5. Edge tracking in hysteresis
   Based on the threshold results, the hysteresis consists of transforming weak pixels into strong ones, if and only if at least one of the pixels around the one being processed is a strong one, as described below:
   ![](https://miro.medium.com/max/1350/1*jnqS5hbRwAmU-sgK552Mgg.png)
   
There is a built-in function for Canny Edge Detection.
```Python
import numpy as np
import cv2
import matplotlib.pyplot as plt
img = cv2.imread('sports.jpg', 0) #read the image in gray-scale.
canny = cv2.Canny(img, 100, 200) 
#We need to provide two threshold values (argument 2 and 3) for hysteresis which is last step. To adjust threshold values add trackbar.
titles =['image', 'canny']
for i in range(2):
    plt.subplot(1,2,i+1)
    plt.imshow(images[i], 'gray')
    plt.title(titles[i])
    plt.xticks([]), plt.yticks([])
```
## 21
# Image Pyramids

**Pyramid**, or **pyramid representation**, is a type of multi-scale signal representation developed by the computer vision, image processing and signal processing communities, in which a signal or an image is subject to repeated **smoothing** and **subsampling**. Pyramid representation is a predecessor to **scale-space representation** and **multiresolution analysis**. With Image pyramids, we can create images with different resolutions.

![Visual Representation of an Image Pyramid with 5 levels](https://upload.wikimedia.org/wikipedia/commons/thumb/4/43/Image_pyramid.svg/300px-Image_pyramid.svg.png)

**There are two kinds of image pyramids:**
### Gaussian Pyramid
In a Gaussian pyramid, subsequent images are weighted down using a Gaussian average (Gaussian blur) and scaled down. Each pixel containing a local average corresponds to a neighborhood pixel on a lower level of the pyramid. This technique is used especially in texture synthesis.
Gaussian pyramids use **cv.pyrDown()** to decrease the resolution of the image and **cv.pyrUp()** to increase the resolution of the image. 
### Laplacian Pyramid
A Laplacian pyramid is very similar to a Gaussian pyramid but saves the difference image of the blurred versions between each levels. Only the smallest level is not a difference image to enable reconstruction of the high resolution image using the difference images on higher levels. This technique can be used in image compression. A level in Laplacian Pyramid is formed by the difference between that level in **Gaussian Pyramid** and **Expanded version of its upper level in Gaussian Pyramid**.

![Image to be changed](https://upload.wikimedia.org/wikipedia/en/thumb/7/7d/Lenna_%28test_image%29.png/220px-Lenna_%28test_image%29.png)

**In this video, the gaussian and laplacian images of the above image are made.**

**The code for the above conversion is:**
```python
import cv2
import numpy as np
img = cv2.imread("lena.jpg")                                                    # reading above image
layer = img.copy()                                                              # making a copy of the image
gaussian_pyramid_list = [layer]                                                 # creating a list of the gaussian images of the given image 

for i in range(6):                                                              # creating a for loop to get all the gaussian transformed versions of the image
    layer = cv2.pyrDown(layer)                                                  # reducing the resolution of the image 
    gaussian_pyramid_list.append(layer)                                         # adding the created image to the list 
    cv2.imshow(str(i), layer)                                                   # command to show the reduced resolution version of the image 

layer = gaussian_pyramid_list[5]                                                # assigning the most reduced image to layer                                            
cv2.imshow('upper level Gaussian Pyramid', layer)                               # showing the most reduced image
laplacian_pyramid_list = [layer]                                                # creating a list for laplacian converted images  

for i in range(5, 0, -1):                                                       # creating an inverse for loop to get all the laplacian transformed versions of the image
    gaussian_expanded = cv2.pyrUp(gaussian_pyramid_list[i])                     # increasing the resolution of the image from the gaussian list which represents the expanded version
    laplacian = cv2.subtract(gaussian_pyramid_list[i-1], gaussian_expanded)     # subtracting a gaussian image having the same resolution from expanded version 
    cv2.imshow(str(i), laplacian)                                               # showing the laplacian image 

cv2.imshow("Original image", img)                                               # showing the original image
cv2.waitKey(0)
cv2.destroyAllWindows()
```
## 22
# Image Blending using Pyramids

One application of Pyramids is Image Blending. For example, in image stitching, you will need to stack two images together, but it may not look good due to discontinuities between images. In that case, image blending with Pyramids gives you seamless blending without leaving much data in the images. One classical example of this is the blending of two fruits, Orange and Apple.

![Apple and Orange image](http://opencv-python-tutroals.readthedocs.org/en/latest/_images/orapple.jpg)

**There are five steps to achieve this:**

1. Load the two images of apple and orange.
2. Find the Gaussian Pyramids for apple and orange (in this particular example, number of levels is 6).
3. From Gaussian Pyramids, find their Laplacian Pyramids.
4. Now join the left half of apple and right half of orange in each levels of Laplacian Pyramids.
5. Finally from this joint image pyramids, reconstruct the original image.

```python
import cv2
import numpy as np
apple = cv2.imread('apple.jpg')                                                                 # read apple image
orange = cv2.imread('orange.jpg')                                                               # read orange image 
print(apple.shape)
print(orange.shape)
apple_orange = np.hstack((apple[:, :256], orange[:, 256:]))                                     # adding the first half of apple and the second half of orange 

                                                                                                # generate Gaussian pyramid for apple
apple_copy = apple.copy()
gp_apple = [apple_copy]
for i in range(6):
    apple_copy = cv2.pyrDown(apple_copy)
    gp_apple.append(apple_copy)


                                                                                                # generate Gaussian pyramid for orange
orange_copy = orange.copy()
gp_orange = [orange_copy]
for i in range(6):
    orange_copy = cv2.pyrDown(orange_copy)
    gp_orange.append(orange_copy)

                                                                                                # generate Laplacian Pyramid for apple
apple_copy = gp_apple[5]
lp_apple = [apple_copy]
for i in range(5, 0, -1):
    gaussian_expanded = cv2.pyrUp(gp_apple[i])
    laplacian = cv2.subtract(gp_apple[i-1], gaussian_expanded)
    lp_apple.append(laplacian)

                                                                                                # generate Laplacian Pyramid for orange
orange_copy = gp_orange[5]
lp_orange = [orange_copy]
for i in range(5, 0, -1):
    gaussian_expanded = cv2.pyrUp(gp_orange[i])
    laplacian = cv2.subtract(gp_orange[i-1], gaussian_expanded)
    lp_orange.append(laplacian)

                                                                                                # Now add left and right halves of images in each level
apple_orange_pyramid = []                                                                       # creating a list for all the images formed by adding the laplacian halves of apple and orange 
n = 0
for apple_lap, orange_lap in zip(lp_apple, lp_orange):                                          # running for loop by creating a zip for laplacian apple and laplacian orange and assigning them to the variables
    n += 1
    cols, rows, ch = apple_lap.shape                                                            # getting number of columns, rows in the shape of apple
    laplacian = np.hstack((apple_lap[:, 0:int(cols/2)], orange_lap[:, int(cols/2):]))           # joining the halves of apple and orange
    apple_orange_pyramid.append(laplacian)                                                      # adding it to the list made above
                                                                                                # now reconstruct
apple_orange_reconstruct = apple_orange_pyramid[0]                                      
for i in range(1, 6):
    apple_orange_reconstruct = cv2.pyrUp(apple_orange_reconstruct)                              # increasing the resolution of the image
    apple_orange_reconstruct = cv2.add(apple_orange_pyramid[i], apple_orange_reconstruct)       # adding the reconstruct version to the image we made by adding the two halves

cv2.imshow("apple", apple)
cv2.imshow("orange", orange)
cv2.imshow("apple_orange", apple_orange)
cv2.imshow("apple_orange_reconstruct", apple_orange_reconstruct)                                # showing the blended image
cv2.waitKey(0)
cv2.destroyAllWindows()
```
## 23
Contours are defined as the line joining all the points along the boundary of an image that are having the same intensity.
OpenCV has _findContour()_ function that helps in extracting the contours from the image. It works best on binary images, so we should first apply thresholding techniques, Sobel edges, etc.
The function returns a Python list of all the contours in the image. Each individual contour is a Numpy array of (x, y) coordinates of boundary points of the object.

``` python
import numpy as np
import cv2

img = cv2.imread('baseball.png')      #reading the image
imgray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)    # coverting the image to grayscale

# Applying threshold, here 0 is code for binary threshold method
ret, thresh = cv2.threshold(imgray, 127, 255, 0)   

# arguments passed are image whose contours are to be found,
# second is contour retreival mode and third countour approximation method
# we need to find contours in this way only defining two variables
contours, hierarchy = cv2.findContours(thresh, cv2.RETR_TREE, cv2.CHAIN_APPROX_NONE)
print("Number of contours = " + str(len(contours)))
print(contours[0])

# for drawing contours on the image arguments passed are:-
# input image, where contours are stored, index of contour use -1 for drawing all contours
# color and thickness
cv2.drawContours(img, contours, -1, (0, 255, 0), 3)
cv2.drawContours(imgray, contours, -1, (0, 255, 0), 3)

# shows the image
cv2.imshow('Image', img)
cv2.imshow('Image GRAY', imgray)
cv2.waitKey(0)
cv2.destroyAllWindows()
```
## 24
# Goal

The aim to read each consecutive frame of a video, find diffrence between them then apply some morphological changes .After this find out contours and enclose them by a rectangle .As result we can easily identify the moving objects in a video .

# Code


~~~python
import cv2
import numpy as np

cap=cv2.VideoCapture('vtest.avi')

ret,frame1 = cap.read()                                                       #reading frame 1
ret,frame2 = cap.read()                                                       #reading frame 2

while cap.isOpened():
    diff=cv2.absdiff(frame1,frame2)                         #cv2. absdiff is a function which helps       
                                                            #in finding the absolute difference between
                                                            #the pixels of the two image arrays. By using
                                                            #this we will be able to extract just the pixels 
                                                            #of the objects that are moving
   
   
   
   
    gray=cv2.cvtColor(diff,cv2.COLOR_BGR2GRAY)                                #convert diff to grayscale mode
    blur=cv2.GaussianBlur(gray,(5,5),0)                                       #blurring the gray image tutorial 18
    _,thresh=cv2.threshold(blur,20,255,cv2.RETR_TREE,cv2.CHAIN_APPROX_SIMPLE)       #using thresholding technique  tutorial 14
    dilated=cv2.dilate(thresh,None,iterations=3)                              # using dilation  tutorial 17
    contours,_=cv2.findContours(dilated,cv2.RETR_TREE,cv2.CHAIN_APPROX_SIMPLE)      #tutorial 23

    for contour in contours:
        (x,y,w,h)=cv2.boundingRect(contour)                                   #calculate  vertex and width and height of the rectangle

        if(cv2.contourArea(contour)<700):                                     #if area of rectangle is less than 700 the it would get neglected
            continue
            

        cv2.rectangle(frame1,(x,y),(x+w,y+h),(0,255,0),2)                     #form a rectangle
        cv2.putText(frame1,"Status: {}".format('Movement'),(10,20),cv2.FONT_HERSHEY_SCRIPT_SIMPLEX,1,(0,0,255),3)
                                                                              #put text tutorial 7

    cv2.imshow("feed",frame1)
#    cv2.imshow("", diff)
    frame1=frame2                                                             #completes a loop
    ret,frame2=cap.read()

    if(cv2.waitKey(40)==27):                                                  #continue until escape key is pressed
        break




cv2.destroyAllWindows()
cap.release()
~~~
## 25
# Detect Simple Geometric Shapes

## Goal

The aim of this code to know how can we detect simple Geometric Shapes using OpenCv library.

## Function:

(1). The functions ```approxPolyDP``` approximate a curve or a polygon with another curve/polygon with less vertices so that the distance between them is less or equal to the specified precision.

* [approxPolyDP](https://docs.opencv.org/2.4/modules/imgproc/doc/structural_analysis_and_shape_descriptors.html#approxpolydp) - ```cv2.approxPolyDP(curve, epsilon, closed[, approxCurve])```

| Argument	 | Function  |
| ---------	 |-----------|
| curve        	 | the contour of the curve                                               |
| epsilon     	 | the approximation accuracy (epsilon = 0.1\*cv2.arcLength(contour,True) |
| closed         | input True if the curve is closed else False				  |

(2). The function ```boundingRect``` calculates the up-right bounding rectangle of a point set, the function calculates and returns the minimal up-right bounding rectangle for the specified point set.

* [boundingRect](https://docs.opencv.org/2.4/modules/imgproc/doc/structural_analysis_and_shape_descriptors.html#boundingrect)
* [boundingRect2](https://docs.opencv.org/3.1.0/dd/d49/tutorial_py_contour_features.html) (refer to point 7) - 
```x,y,w,h = cv2.boundingRect(points)```

## Code

```python
import cv2                                                                                  
import numpy as np                                                                          
                                                                                            
img = cv2.imread('shapes.jpg')                                                             
imgGrey = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)                                             
_, thrash = cv2.threshold(imgGrey, 240, 255, cv2.THRESH_BINARY)                             
contours, _ = cv2.findContours(thrash, cv2.RETR_TREE, cv2.CHAIN_APPROX_NONE)                

for contour in contours:                                                                    
    approx = cv2.approxPolyDP(contour, 0.01*cv2.arcLength(contour, True), True)             #this method approximates a polygonal curve with a specific precision
    cv2.drawContours(img, [approx], 0, (0, 0, 0), 5)                                        
    x = approx.ravel()[0]                                                                   #coordinates of the shape to put Text
    y = approx.ravel()[1] - 15                                                              #ravel is a method to get coordinates
    if len(approx) == 3:                                                                    
        cv2.putText(img, 'Triangle', (x, y), cv2.FONT_HERSHEY_COMPLEX, 0.5, (0, 0, 0))      
    elif len(approx) == 4:                                                                  
        x1, y1, w, h = cv2.boundingRect(approx)                                             #using boundingRect method get Width and
        aspectRatio =float(w)/h                                                             #Height and check the aspect ratio
        print(aspectRatio)                                                                  
        if aspectRatio >= 0.95 and aspectRatio<=1.05:                                       #considering noises check for a square or rectangle
            cv2.putText(img, 'Square', (x, y), cv2.FONT_HERSHEY_COMPLEX, 0.5, (0, 0, 0))    
        else:                                                                               
            cv2.putText(img, 'Rectangle', (x, y), cv2.FONT_HERSHEY_COMPLEX, 0.5, (0, 0, 0)) 
    elif len(approx) == 5:                                                                  
        cv2.putText(img, 'Pentagon', (x, y), cv2.FONT_HERSHEY_COMPLEX, 0.5, (0, 0, 0))      
    elif len(approx) == 10:                                                                 
        cv2.putText(img, 'Star', (x, y), cv2.FONT_HERSHEY_COMPLEX, 0.5, (0, 0, 0))          
    else :                                                                                  
        cv2.putText(img, 'Circle', (x, y), cv2.FONT_HERSHEY_COMPLEX, 0.5, (0, 0, 0))        

cv2.imshow("shapes", img)                                                                   
cv2.waitKey(0) & 0xFF                                                                       
cv2.destroyAllWindows()                                                                     
```
## References

* [YouTube Video](https://youtu.be/mVWQNeY1Pb4)
* [Structural Analysis and Shape Descriptors](https://docs.opencv.org/2.4/modules/imgproc/doc/structural_analysis_and_shape_descriptors.html#approxpolydp)
## 26

~~~python
import numpy as np
import cv2
from matplotlib import pyplot as plt

img=cv2.imread("lena.jpg")
img1=cv2.imread("lena.jpg",0)
hist=cv2.calcHist((img1),[0],None,[256],[0,256])#first argument is source,second argument is channel number
                                                # to find the histogram of full image we have to give third argument as none
                                                #fourth argument is bin count,fifth argument will be the min and max range on x axis

b,g,r=cv2.cv2.split(img)#split the img array into three diffrent channels

cv2.imshow("img",img)
#cv2.imshow("b",b)
#cv2.imshow("g",g)
#cv2.imshow("r",r)


plt.hist(b.ravel(),256,[0,256])#b.ravel is used to flatten the b array
                               #second argument is the maximum value of pixel
                               #third argument is the range
plt.hist(g.ravel(),256,[0,256])
plt.hist(r.ravel(),256,[0,256])

plt.show()

cv2.waitKey(10000)
cv2.destroyAllWindows()


~~~
## 27
# Template Matching Using OpenCV
```Python
#code for template matching using opencv
#Template matching is searching and finding location of template image in original reference image

import cv2                                                           #importing opencv and 
import numpy as np                                                   #numpy libraries

img = cv2.imread("messi.jpg")                                        #reading the original image 'messi.jpg'
grey_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)                     #converting img into grayscale image
template = cv2.imread("messi_face.jpg", 0)                           #reading the template image 'messi_face.jpg' 

w, h = template.shape[::-1]                                          #assigning width and height of template image into 
                                                                     #variables w and h.
                                                                     
res = cv2.matchTemplate(grey_img, template, cv2.TM_CCORR_NORMED )    #template matching of 'template'(2nd arg) with respect 
                                                                     #'grey_img'(1st arg) using a method called 
                                                                     #'TM_CCORR_NORMED'(3rd arg)
                                                                     
print(res)                                                           #prints matrix containing decimal values
                                                                     #we have to extract all the brightest points from res 
                                                                     #matrix.The point where template images's topleft corner
                                                                     #matches with original image,res will have its brightest
                                                                     #point there.
                                                                     
threshold = 0.99;                                                    #filtering out brightest points inside matrix by giving 
loc = np.where(res >= threshold)                                     #threshold value.Increasing threshold value will give 
                                                                     #less no.of values and thus we can find the brightest 
                                                                     #point.
print(loc)                                                           #printing the desired brightest points.        

for pt in zip(*loc[::-1]):                                           #for loop is used in case we get multiple no.of  
                                                                     #desired templates in same image.
    cv2.rectangle(img, pt, (pt[0] + w, pt[1] + h), (0, 0, 255), 2)   #drawing rectangle around the original image,same as the 
                                                                     #same as the size of template with img(1st arg),
                                                                     #iterating variable(2nd arg),width and height(3rd arg),
                                                                     #red colour(4th arg),thickness(5th arg)

cv2.imshow("img", img)                                               #displaying of image with rectangle drawn.
cv2.waitKey(0)
cv2.destroyAllWindows()

#increasing threshold will give more no. of points on x axis and y axis.Thus the same rectangle is drawn multiple times.
#decreasing threshold value will give us exact points resulting in precision to draw the rectangle.
#Other than 'cv2.TM_CCORR_NORMED ' method ,there are another template matching methods also.

```
## 28
# HOUGH TRANSFORM

Hough Transform is basically a feature extraction technique used in image analysis, computer vision and digital image processing. The aim is to find instances (though imperfect) of objects within a certian class of shapes governed by some sort of voting procedure. 

A very high level glimpse is that we transform our co-ordinates to a new parametric space. Using this, the objects that we are trying to search for are found as the local maxima in accumulator space contructed by the algorithm. 

Initially, the transform was aimed at extracting probable lines in the image but now it has been extended to identify some classes of shapes. 

One could argue that the edge detector can do the job of finding straight lines. Indeed, to some extent that works but that has certain limitations - 
* There can be missing points/pixels on the desired curve obtained using the edge detector.
* There can be devations between the actual curves and the desired curves that can lead to some sort of non-detectability. 

So, Hough Transform Algorithm helps to deal with these imperfections inherently present in the data or caused by the edge detector. This algorithm develops some sort of grouping of the edge pixels and based on the parameters set, tries to see if the desired object can be traced in the image depending on the explicit voting procedure that is undertaken. 

Let's now delve into the technical details of the Transform and the corresponding algorithm.

### Cartesian Form 
  The general equation of a straight line is given as by : *y = mx + c*. Now think about transforming it to the *(m, c)* space. How does the line look in the *(m, c)* space? That look like a point. And any general point on the line is a line in the *(m, c)* space. This can be realised by representing the equation in the following format : *c = -mx + y*, so the slope of this line is *x* and the intercept being *y*.  
  So this now gives a general idea to see if a line is present in the image or not. For all points on the edge, we draw the corresponding line on the *(m, c)* space and in the accumulator space, we obtain the votes i.e. the number of lines through it, of each co-ordinate of the *(m, c)* space. If it is more than some threshold, then a line with that *(m, c)* is said to exist in the image and that completes our job. 


  
  ![640px-Hough_transform_diagram svg](https://user-images.githubusercontent.com/55907159/79689144-7f2fab80-8270-11ea-80e4-2acc97e5711f.png)



  The above diagram gives some insights to what is actually going on. Bascially, this shows that the *(m, c)* with the highest number of votes is probably the object we are looking for. In this case, we see that (405, 60&deg;) is somewhat and optimal choice for the *(r, &theta;)* of the line.

  We can extend this idea to the *(r, &theta;)* space i.e. the Polar form using the Hesse Normal Form of a line : *r = xcos&theta; + ysin&theta;*. 

### Polar Form 
  The general equation of a line in normal form is : *r = xcos&theta; + ysin&theta;*. Now, each point on the line transformed into the *(r, &theta;)* space gives some sort of a sinusoidal curve in contrast to the line in the *(m, c)* space.
  In the same way as the cartesian form, we draw the sinusoids in the *(r, &theta;)* space and in the accumulator space, we obtain the votes i.e. the number of curves through it, of each co-ordinate of the *(r, &theta;)* space. If it is greater than some threshold value, then a line with *(r, &theta;)* exists on the image. 

The polar form is preferred over the cartesian form because of the fact that for vertical lines, the slope blows up and hence it is not possible to represent the line in *(m, c)* space, but that limitation doesn' show up in polar form.  

---

### References

* [Wikipedia](https://en.wikipedia.org/wiki/Hough_transform)
* [Youtube Video](https://www.youtube.com/watch?v=7m-RVJ6ABsY&list=PLS1QulWo1RIa7D1O6skqDQ-JZ1GGHKK-K&index=32)
## 29

# Hough Line Transform

## Goal

The aim of this code is to demonstrate the usage of the Hough Line Transform function of the opencv library. The function identifies the lines based on **votes** threshold set by the user. The code results in the display of the infinite length lines obtained by applying the Hough Line function to the given image.

## Requisites

#### Concepts - 
* [Hough Transform Theory](https://github.com/MananKGarg/SOC_20_Virtual_Keyboard/blob/master/SoC_OpenCV-master/28.%20%5BAnkit%5D%20Hough%20Line%20Transform%20Theory.md) (refer to the given link)
#### Functions - 
* [Hough Line Transform](https://docs.opencv.org/2.4/doc/tutorials/imgproc/imgtrans/hough_lines/hough_lines.html) - `cv2.HoughLines(edges, resolution_of_r, resolution_of_theta, min_votes)`

  | Argument              |    Function     |
  | -------------         | --------------- |
  | edges                 | The edges detected using Canny Edge Detector or any other technique |
  | resolution_of_r       | The resolution of the *r* parameter of the line                     |
  | resolution_of_&theta; | The resolution of the *&theta;* parameter of the line               |
  | min_votes             | The minimum threshold on the number of votes of the line to be displayed|
  
* [Line](https://github.com/MananKGarg/SOC_20_Virtual_Keyboard/blob/master/SoC_OpenCV-master/5.%20%5BPallavi%5D%20Draw%20geometric%20shapes%20on%20images%20using%20Python%20OpenCV.md) (refer to the given link)
* [Trackbar](https://github.com/MananKGarg/SOC_20_Virtual_Keyboard/blob/master/SoC_OpenCV-master/12.%20(Nirmal)How%20to%20Bind%20Trackbar%20To%20OpenCV%20Windows.md) (refer to the given link)

## Code Insights

* The image is read in the same format as the image and is then converted to Gray scale mode so that the Canny Edge Detection can applied to the image.
* A trackbar is created to see the effect of the threshold on the lines detected by the Hough Line Function, so that appropriate threshold can be set. 
* The image is passed to the Hough Line Function and the detected lines are displayed on the image itself.
* The image is then read and displayed continuously to reflect any changes that happen to the image.
* To draw the line, we first have to obtain the end points of the line in *(x,y)* form from the *(r, &theta;)* form so that it can be passed an argument to the line function.
* The lines are then drawn using the line function of the opencv library.

## Code

```python 
import cv2 as cv 
import numpy as np 				

def nothing(x):                                                 # A dummy callback function for the Trackbar position change
	pass                                                    # Do nothing inside the function

img = cv.imread('sudoku.png')                                   # Reading the image and storing it in the variable 'img'
gray = cv.cvtColor(img, cv.COLOR_BGR2GRAY)                      # Converting the image to grayscale mode

edges = cv.Canny(gray, 50, 150, apertureSize = 3)               # Canny Edge Detection
cv.imshow('edges', edges)                                       # Display the edges obtained using the above algorithm

cv.namedWindow('image')                                         # Creating a named window 
cv.createTrackbar('threshold', 'image', 200, 300, nothing)      # Creating a Trackbar with 'threshold' label

while(1):							# To display the image consistently
	cv.imshow('image', img)					# The generated lines are displayed on the image itself
	k = cv.waitKey(1)					# Waits for the user to press a key for 1 ms.
	if k == 27:					        # If 'escape' key is pressed, the loop is terminated
		break
	img = cv.imread('sudoku.png')				# The image is read into the img variable

	thresh = cv.getTrackbarPos('threshold', 'image')	# Obtaining the value of the Trackbar
	lines = cv.HoughLines(edges, 1, np.pi/180, thresh)	# Obtaining the lines using the HoughLines Function

	for line in lines:					# Iterating over the lines obtained
		rho, theta = line[0]			        # Obtaining the r, theta of the line
		a = np.cos(theta)				# Evaluating cos theta
		b = np.sin(theta)				# Evaluating sin theta
		x0 = a*rho					# Obtaining the perpendicular distant x co-ordinate
		y0 = b*rho					# Obtaining the perpendicular distant y co-ordinate
		x1 = int(x0 + 1000*(-b))			# Specifying x co-ordinate of one end of the line 
		y1 = int(y0 + 1000*a)				# Specifying y co-ordinate of one end of the line
		x2 = int(x0 + 1000*b)				# Specifying x co-ordinate of the other end of the line
		y2 = int(y0 + 1000*(-a))			# Specifying y co-ordinate of the other end of the line
		cv.line(img, (x1,y1), (x2,y2), (0,0,255), 2)	# Drawing the line

cv.destroyAllWindows()						# Closing all the windows and terminate the program
```
## References

* [OpenCV Docs](https://docs.opencv.org/3.4/d9/db0/tutorial_hough_lines.html)
* [Youtube Video](https://www.youtube.com/watch?v=gbL3XKOiBvw&list=PLS1QulWo1RIa7D1O6skqDQ-JZ1GGHKK-K&index=33)

## 30

# Probabilistic Hough Line Transform

## Goal

The aim of this code is to demonstrate the usage of the Probabilistic Hough Line Transform function of the opencv library. The function identifies the lines based on certain parametric thresholds set by the user. The code results in the display of the finite length lines obtained by applying the Probabilistic Hough Line function to the given image.

## Requisites

#### Concepts - 
* [Hough Transform Theory](https://github.com/MananKGarg/SOC_20_Virtual_Keyboard/blob/master/SoC_OpenCV-master/28.%20%5BAnkit%5D%20Hough%20Line%20Transform%20Theory.md) (refer to the given link)

#### Functions - 
* [Probabilistic Hough Line Transform](https://docs.opencv.org/2.4/doc/tutorials/imgproc/imgtrans/hough_lines/hough_lines.html) - `cv2.HoughLinesP(edges, resolution_of_r, resolution_of_theta, min_votes, min_length, max_gap)`

  | Argument              |    Function     |
  | -------------         | --------------- |
  | edges                 | The edges detected using Canny Edge Detector or any other techniques|
  | resolution_of_r       | The resolution of the *r* parameter of the line                     |
  | resolution_of_&theta; | The resolution of the *&theta;* parameter of the line               |
  | min_votes             | The minimum threshold on the number of votes of line to be detected |
  | min_length            | Lines shorter than this length are rejected                         |
  | max_gap               | Allowed distance between two points on a line is max_gap to link them|
  
* [Line](https://github.com/MananKGarg/SOC_20_Virtual_Keyboard/blob/master/SoC_OpenCV-master/5.%20%5BPallavi%5D%20Draw%20geometric%20shapes%20on%20images%20using%20Python%20OpenCV.md) (refer to the given link)
* [Trackbar](https://github.com/MananKGarg/SOC_20_Virtual_Keyboard/blob/master/SoC_OpenCV-master/12.%20(Nirmal)How%20to%20Bind%20Trackbar%20To%20OpenCV%20Windows.md) (refer to the given link)

## Code Insights

* The image is read in the same format as the image and is then converted to Gray scale mode so that the Canny Edge Detection can applied to the image.
* Two trackbars are made to fine tune the parameteres of min_length and max_gap, so that the desired representation can be obtained.
* The image is passed to the Probabilistic Hough Line Function and the detected lines are displayed on the image itself.
* The image is then read and displayed continuously to reflect any changes that happen to the image.
* The lines are then drawn using the line function of the opencv library.

## Special Note

If carefully observed, the only difference between the codes of Hough Line Transform and Probabilistic Hough Line Transform is that of the function, the rest of the code is exactly the same. The thing is, the standard function draws the lines of inifinte length, whereas the probabilistic model draws lines of finite length. It somehow manages to draw the probable lines present on the image in terms of the terminal points of the line unlike the only *(r, &theta;)* values obtained in the standard model. So, there lies a big difference between the two models.

## Code

```python 
import cv2 as cv               
import numpy as np 

def nothing(x):				                # Dummy callback function for the Trckbar position change
	pass					        # Do nothing inside the function
img = cv.imread('sudoku.png')		                # Read the image 'sudoku.png' from the directory
gray = cv.cvtColor(img, cv.COLOR_BGR2GRAY)		# Convert the image to grayscale mode.
edges = cv.Canny(gray, 50, 150, apertureSize = 3)	# Apply the Canny Edge Detection
cv.imshow('edges', edges)				# Displays the image on the 'edge' window

cv.namedWindow('image')				        # Create a named window with name 'image'

cv.createTrackbar('min', 'image', 100, 100, nothing)	# Create a trackbar with 'min' label
cv.createTrackbar('max', 'image', 10, 100, nothing)	# Create a trackbar with 'max' label

while(1):						# To display the image consistently
	cv.imshow('image', img)			        # Displays the image on the 'image' window
	k = cv.waitKey(1)				# Waits for the user to press some key
	if k == 27:					# If 'escape' key is pressed, the loop is terminated
		break
	img = cv.imread('sudoku.png')	                # The image is read into the 'img' variable

	m1 = cv.getTrackbarPos('min', 'image')		# The trackbar position of the 'min' label is obtained
	m2 = cv.getTrackbarPos('max', 'image')		# The trackbar position of the 'max' label is obtained
	lines = cv.HoughLinesP(edges, 1, np.pi/180, 100,minLineLength  = m1, maxLineGap = m2)	# The probabilistic Hough Lines Function is used.

	for line in lines:				# Iterating over the lines
		x1,y1,x2,y2 = line[0]			# The extreme co-ordinates of the line are obtained
		cv.line(img, (x1,y1), (x2,y2), (0,255,0), 2)	# The line is drawn on the image itself

cv.destroyAllWindows()				        # All the windows are destroyed and the program is terminated.
```

## References 

* [Youtube Video](https://www.youtube.com/watch?v=rVBVqVmHtfc&list=PLS1QulWo1RIa7D1O6skqDQ-JZ1GGHKK-K&index=34)
* [OpenCV Doc](https://docs.opencv.org/3.4/d9/db0/tutorial_hough_lines.html)
## 31
# Road Lane Detection - 1
##### Reference : [https://www.youtube.com/watch?v=yvfI4p6Wyvk](https://www.youtube.com/watch?v=yvfI4p6Wyvk)
_________________________________________________________________________________________________________________________________________
### Aim : To narrow our field of view to our Region of Interest (ROI)
##### Note: In this code, our ROI is the triangular region b/n the center and the two botton vertices of the rectangle. 
##### New Functions:
1. `b = np.zeros_like(a)` - Makes b of same shape as a, filled with zeros.
2. ##### `cv2.fillPoly(img, pts, color)` 
##### Arguments :
* img : This is the source image, type - <numpy array>
* pts : This is the set of vertices of our polygon, type - <list(<tuples>)>
* color : This is what we want to fill our polygon with. type - <tuple>
##### Note: While using this function, you **needn't** assign this to some variable, the function modifies the img variable itself.
#### Code:
```python
import matplotlib.pylab as plt
import cv2
import numpy as np

image = cv2.imread('road.jpg')                                                      #y'all know what dis does
image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)                                      #and this too

print(image.shape)                                                  #prints the dimentions of our image array
height = image.shape[0]                                             #saving height in .... height!!
width = image.shape[1]                                              #saving width in .... width!!

region_of_interest_vertices = [ (0, height), (width/2, height/2), (width, height) ]
                                                #we have stored the coordinates of our ROI vertices in a list

def region_of_interest(img, vertices):
    mask = np.zeros_like(img)                           #array with dimentions of image initialised with zeros 
    channel_count = img.shape[2]                        #img.shape[2] represents the number of color channels (= 3 here)
    match_mask_color = (255,) * channel_count           #(255,)*3 returns (255, 255, 255)
    cv2.fillPoly(mask, vertices, match_mask_color)      #This is explained above
    masked_image = cv2.bitwise_and(img, mask)           #makes all pixels out of our ROI 0
    return masked_image

cropped_image = region_of_interest(image, np.array([region_of_interest_vertices], np.int32),)
                                                #we specify the data type to prevent misunderstandings...
plt.imshow(cropped_image)
plt.show()
```
_____________________________________________________________________________________________________________________________________
```

## 32
# Road Lane Detection - 2
##### Reference : [https://www.youtube.com/watch?v=yvfI4p6Wyvk](https://www.youtube.com/watch?v=G0cHyaP9HaQ)
_________________________________________________________________________________________________________________________________________
### Aim : To obtain lines inside our ROI.
##### Note: In this code, our ROI is the triangular region b/n the center and the two botton vertices of the rectangle. 
##### Functions Used:
1. [Canny edge detector](https://github.com/MananKGarg/SOC_20_Virtual_Keyboard/blob/master/SoC_OpenCV-master/20.%20%20(Aanal)%20Canny%20Edge%20Detection%20in%20OpenCV.md)
2. [Probabilistic Hough line transform](https://github.com/MananKGarg/SOC_20_Virtual_Keyboard/blob/master/SoC_OpenCV-master/30.%20%5BAnkit%5D%20Probabilistic%20Hough%20Transform%20using%20HoughLinesP.md) 
3. [Add Weighted](https://github.com/MananKGarg/SOC_20_Virtual_Keyboard/blob/master/SoC_OpenCV-master/10.%20(Aashuraj_Hassani)%20cv.split%2C%20cv.merge%2C%20cv.resize%2C%20cv.add%2C%20cv.addWeighted%2C%20ROI.md)

##### Code:
```python
import matplotlib.pylab as plt
import cv2
import numpy as np

def region_of_interest(img, vertices):
    mask = np.zeros_like(img)                                           
    #channel_count = img.shape[2]
    match_mask_color = 255                                                         #done in the previous video
    cv2.fillPoly(mask, vertices, match_mask_color)
    masked_image = cv2.bitwise_and(img, mask)
    return masked_image

def drow_the_lines(img, lines):
    img = np.copy(img)                                                             #copy to protect original image
    blank_image = np.zeros((img.shape[0], img.shape[1], 3), dtype=np.uint8)        #blank image with same dimentions

    for line in lines:
        for x1, y1, x2, y2 in line:                                                #coordinates of line ends
            cv2.line(blank_image, (x1,y1), (x2,y2), (0, 255, 0), thickness=10)     #draw this line

    img = cv2.addWeighted(img, 0.8, blank_image, 1, 0.0)                           #Add this to original to see the lines there
    return img

image = cv2.imread('road.jpg')
image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
print(image.shape)
height = image.shape[0]
width = image.shape[1]                                                            #Done in the previous video
region_of_interest_vertices = [
    (0, height),
    (width/2, height/2),
    (width, height)
]
gray_image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
canny_image = cv2.Canny(gray_image, 100, 200)
cropped_image = region_of_interest(canny_image,
                np.array([region_of_interest_vertices], np.int32),)
lines = cv2.HoughLinesP(cropped_image,                                           #lines has our coordinates as a nested list
                        rho=6,
                        theta=np.pi/180,
                        threshold=160,
                        lines=np.array([]),
                        minLineLength=40,
                        maxLineGap=25)
image_with_lines = drow_the_lines(image, lines)
plt.imshow(image_with_lines)
plt.show()
```
## 33
# Aim:
Build a lane-detection algorithm fuelled entirely by Computer Vision.
```python
    import matplotlib.pylab as plt
    import cv2
    import numpy as np

def region_of_interest(img, vertices):
    mask = np.zeros_like(img)
    #channel_count = img.shape[2]
    match_mask_color = 255
    cv2.fillPoly(mask, vertices, match_mask_color)
    masked_image = cv2.bitwise_and(img, mask)
    return masked_image

def drow_the_lines(img, lines):
    img = np.copy(img)
    blank_image = np.zeros((img.shape[0], img.shape[1], 3), dtype=np.uint8)
    
    for line in lines:
        for x1, y1, x2, y2 in line:
            cv2.line(blank_image, (x1,y1), (x2,y2), (0, 255, 0), thickness=10)  
       
    img = cv2.addWeighted(img, 0.8, blank_image, 1, 0.0)
    return img

def process(image):                                      #This function overwrites a frame with the the detected lines
    print(image.shape)
    height = image.shape[0]
    width = image.shape[1]
    region_of_interest_vertices = [
        (0, height),
        (width/2, height/2),
        (width, height)
    ]
    gray_image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
    canny_image = cv2.Canny(gray_image, 100, 120)        # canny parameters altered for better output quality
    cropped_image = region_of_interest(canny_image,
                    np.array([region_of_interest_vertices], np.int32),)
    lines = cv2.HoughLinesP(cropped_image,
                            rho=2,
                            theta=np.pi/180,
                            threshold=50,                 #parameters altered for better output quality
                            lines=np.array([]),
                            minLineLength=40,
                            maxLineGap=100)
    image_with_lines = drow_the_lines(image, lines)
    return image_with_lines

cap = cv2.VideoCapture('test.mp4')              # Video Capture with its parameter as the test video

while cap.isOpened():                            #checks if cap is opened
    ret, frame = cap.read()                      #reading the frame
    frame = process(frame)                       # overwriting frame with the frame with lane lines
    cv2.imshow('frame', frame)
    if cv2.waitKey(1) & 0xFF == ord('q'):        #exits if user presses 'q'
        break

cap.release()
cv2.destroyAllWindows()
```
## 34
#Aim:
Circle Detection using OpenCV Hough Circle Transform

```python
import numpy as np
import cv2 as cv
img = cv.imread('smarties.png')                                   # reads image
output = img.copy()                         
gray = cv.cvtColor(img, cv.COLOR_BGR2GRAY)
gray = cv.medianBlur(gray, 5)                                    #blurring image because hough circle method works better with blur images
circles = cv.HoughCircles(gray, cv.HOUGH_GRADIENT, 1, 20,
                          param1=50, param2=30, minRadius=0, maxRadius=0)
                #HoughCircles has a few parameters (img,detection method,dp,min_dist,parameter1,parameter2,minRadius,maxRadius)
                          
detected_circles = np.uint16(np.around(circles))          #integer conversion of x,y,radius
for (x, y ,r) in detected_circles[0, :]:
    cv.circle(output, (x, y), r, (0, 0, 0), 3)            #marks the boundary of the detected circles
    cv.circle(output, (x, y), 2, (0, 255, 255), 3)        #marks the center of the circles by making a very small circle at the center


cv.imshow('output',output)
cv.waitKey(0)
cv.destroyAllWindows()

```
## 35
# Object Detection using Cascade Classifiers

- It is a machine learning based approach where a cascade function is trained from a lot of positive and negative images. It is then used to detect objects in other images.

- Before extracting features, the algorithm needs a lot of positive images and negative images to train the classifier. For this, Haar features are used. A Haar feature considers adjacent rectangular regions at a specific location in a detection window, sums up the pixel intensities in each region and calculates the difference between these sums.  

- Instead of repeating this for each & every pixel, an integral image is used. However large your image, it reduces the calculations for a given pixel to an operation involving just four pixels.

- Most of the calculated features are irrelevant. So, to select the best features, we apply each & every feature on all the training images. Each image is given an equal weight in the beginning. After each classification, weights of misclassified images are increased. Then the same process is done. New error rates & weights are calculated. The process is continued until the required accuracy or error rate is achieved or the required number of features are found.

- The final classifier is a weighted sum of these weak classifiers. The paper says even 200 features provide detection with 95% accuracy. Their final setup had around 6000 features.

- In an image, most of the image is non-face region. If it is not a face region, discard it in a single shot and focus on regions where there can be a face. This way, we spend more time checking possible face regions.

- For this they introduced the concept of Cascade of Classifiers. Instead of applying all 6000 features on a window, the features are grouped into different stages of classifiers and applied one-by-one. If a window fails the first stage, discard it. If it passes, apply the second stage of features and continue the process. The window which passes all stages is a face region.

# OpenCV for Object Detection
* OpenCV comes with a trainer as well as a detector.For the face classifier, the cascade function takes an grayscale image as an input and gives the location & size of face as output
* ` objects = cv2.CascadeClassifier.detectMultiScale(image, scaleFactor, minNeighbors) `
   | Keyword | Specification |
   | ------- | --------------|
   |objects| Vector of rectangles where each rectangle contains the detected object, the rectangles may be partially outside the original image.|
   |image | Matrix of the type CV_8U containing an image where objects are detected.|
   |scaleFactor| Parameter specifying how much the image size is reduced at each image scale|
   |minNeighbors| Parameter specifying how many neighbors each candidate rectangle should have to retain it.|
   
# Code
```python
import cv2
                                                                            
face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml') # Defining an object for the face classifier
cap = cv2.VideoCapture('test.mp4')                                          # Add the location of video file here

while cap.isOpened():
    _, img = cap.read()
 
    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)                            # The classifier needs a grayscale image
    faces = face_cascade.detectMultiScale(gray, 1.1, 4)                     # Gives the list of tuples containing location & size of faces

    for (x, y , w ,h) in faces:                                             # (x, y): Position of Face
        cv2.rectangle(img, (x,y), (x+w, y+h), (255, 0 , 0), 3)              # (w, h): Width & Height of Face
                                                                            # Draws rectangle around face(s)
    # Display the output
    cv2.imshow('img', img)
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

cap.release()
```
# Resources
- Repository for [HaarCascade files](https://github.com/opencv/opencv/tree/master/data/haarcascades)
- Video lecture on [Face detection and tracking](https://www.youtube.com/watch?v=WfdYYNamHZ8)
## 36
# Eye Detection in OpenCV

- Similar to face detection, there is a pre-trained cascade classifier for eye detection as well.
- Here, we first detect the face region and then detect the position of eyes in these regions.

# Code
```python
import cv2

face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')  # Define object for face classifier
eye_cascade = cv2.CascadeClassifier('haarcascade_eye_tree_eyeglasses.xml')   # Define object for eye classifier
cap = cv2.VideoCapture('test.mp4')                                           # Add the location for video file here

while cap.isOpened():
    _, img = cap.read()
    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
    faces = face_cascade.detectMultiScale(gray, 1.1, 4)                      # Gives the list of tuples containing location & size of faces

    for (x, y , w ,h) in faces:
        cv2.rectangle(img, (x,y), (x+w, y+h), (255, 0 , 0), 3)               # Draws rectangle around face(s)
        roi_gray = gray[y:y+h, x:x+w]                                        # Region of image containing face in grayscale
        roi_color = img[y:y+h, x:x+w]                                        # Region of image containing face in color
        eyes = eye_cascade.detectMultiScale(roi_gray)                        # Gives the list of tuples containing location & size of eyes
        for (ex, ey ,ew, eh) in eyes:                                        # (ex, ey): Upperleft corner of eye-rectangle
            cv2.rectangle(roi_color, (ex,ey), (ex+ew, ey+eh), (0, 255, 0), 5)# (ew, eh): Width & Height of eye
                                                                             # Draws rectangle around eyes
    # Display the output
    cv2.imshow('img', img)
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

cap.release()
```

# Resources
- Repository for [HaarCasacde files](https://github.com/opencv/opencv/tree/master/data/haarcascades)
## 37
# Detect Corners with Harris Corner Detector
Corners- Are the region in image with large intensity variation in all directions. Ths method includes three main steps:
STEP 1. It determines which windows (small image patches) produce very large variations in intensity when moved in both X and Y directions (i.e. gradients).
STEP 2. With each such window found, a score R is computed.
STEP 3. After applying a threshold to this score, important corners are selected & marked.

### Mathematical overview
STEP 1 :How do we determine windows which produce large variations?
Let a window (the center) be located at position (x,y). Let the intensity of the pixel at this location be I(x,y). If this window slightly shifts to a new location with displacement (u,v), the intensity of the pixel at this location will be I(x+u,y+v). Hence [I(x+u,y+v)-I(x,y)] will be the difference in intensities of the window shift. For a corner, this difference will be very high. Hence, we maximize this term by differentiating it with respect to the X and Y axes. Let w(x,y) be the weights of pixels over a window (Rectangular or a Gaussian) Then, E(u,v) is defined as :

   ![](https://miro.medium.com/max/810/0*v4pgxvEFE8JvroJv.png)
    
Weighted sum multiplied by the intensity difference for all pixels in a window.
Now, computing E(u,v) by the above formula will be really, really slow. Hence, we use Taylor series expansion (only the 1rst order).

![](https://miro.medium.com/max/1400/1*2f_Yy0iYm62xfJItAte-CQ.png)

STEP 2: Now that we know how to find windows with large variations, how do we select the ones with suitable corners? It was estimated that the eigenvalues of the matrix can be used to do this. Thus, we calculate a score associated with each such window.

![](https://miro.medium.com/max/283/0*oahBtth2YSWxNa-Z.png)

![Score for classifying into flat region/edge/corner.](https://miro.medium.com/max/1400/1*tKBvNLJ22UBzWm9JWcB3Tg.png)

STEP 3: Depending on the value of R, the window is classified as consisting of flat, edge, or a corner.(Finally 😛) A large value of R indicates a corner, a negative value indicates an edge. Also, to pick up the optimal corners, we can use non-maximum suppression.

```Python
import numpy as np
import cv2
img = cv2.imread("crossboard.jpg") 
cv2.imshow('image', img)
img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) #Built-in function for harris detector takes gray scale image (in float32) and grayscale also gives better results. 
img_gray = np.float32(img_gray)
dst = cv2.cornerHarris(img_gray, 2, 3, 0.04) #second parameter is block-size that is window-size is block_size square. third parameter is ksize that is the aperture size used for Sobel (used in step 1) and fourth is k which is harris free parameter in the equation. 
dst = cv2.dilate(dst, None) # dilate to get better results.

img[dst>0.01*dst.max()] = [0, 0, 255] #revert back to old image and mark the corners.
cv2.imshow('corner detection', img)

if cv2.waitKey(0) and 0xff==27:
    cv2.destroyAllWindows()```
```
## 38
# SHI THOMSAI CORNER DETECTION

It is better method than Harris Corner detction method with giving us the option to choose number of corners to be detcted.

```python
import numpy as np
import cv2 as cv

#reading the image argument passed is the name of the image file to be read
img = cv.imread('pic1.png')

# Coverting image to grayscale as cv2.goodfeaturesToTrack takes grayscale image as an input.
# cv2.cvtColor can be used for converting from one color format to another like BGR2RGB, BGR2GRAY, BGR2HSV
gray = cv.cvtColor(img, cv.COLOR_BGR2GRAY)

# Function which uses Shi Thomasi Corner Detction method
# It takes no. of arguments first the grayscale image, second number of corners you want to detect
# third quality which decides that it is corner or not (preferable 0.01)
# fourth minimum Euclidean distance between the detected corners
corners = cv.goodFeaturesToTrack(gray, 100, 0.01, 10)

# used to convert into 64 bit int
corners = np.int0(corners)

for i in corners:
     
    # ravel methods return contingous 1D array if it is 2D or 3D array
    x, y = i.ravel()        
    
    # To draw circle on the image
    # arguments passed are (source image, centre of circle in form of tuple, radius of circle, color in BGR format, thickness use negative to fill the circle)
    cv.circle(img, (x, y), 3, [255, 255, 0], -1)

# Shows the image to the user with first argument name of the window and second image source
cv.imshow('Shi-Tomasi Corner Detector', img)

if cv.waitKey(0) & 0xff == 27:
    cv.destroyAllWindows()
```
